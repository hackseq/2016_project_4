{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAMTOOLS MPILEUP ON MULTIPLE BAMS\n",
    "================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! samtools --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See ``mpileup`` [manual](http://samtools.sourceforge.net/mpileup.shtml).\n",
    "``samtools mpileup`` requires:\n",
    "- an index reference fasta\n",
    "- indexed bam files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input bam files used in this example are assembled on hg19. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create a commands file for parallelization. We are using chromosome 22 as an example here. \n",
    "Parameters similar to GATK's default read filters were applied:\n",
    "- depth 1000000 \n",
    "- mapping quality 20 ([GATK haplotype caller](https://software.broadinstitute.org/gatk/gatkdocs/org_broadinstitute_gatk_tools_walkers_haplotypecaller_HaplotypeCaller.php) default)\n",
    "- base quality 10 ([GATK haplotype caller](https://software.broadinstitute.org/gatk/gatkdocs/org_broadinstitute_gatk_tools_walkers_haplotypecaller_HaplotypeCaller.php) default)\n",
    "- exclude read unmapped, not primary alignment, read fails platform/vendor quality checks, read is PCR or optical duplicate ([GATK haplotype caller](https://software.broadinstitute.org/gatk/gatkdocs/org_broadinstitute_gatk_tools_walkers_haplotypecaller_HaplotypeCaller.php) default)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "REFERENCE=\"/hackathon/Hackathon_Project_4/REFERENCE_GENOME/hg19.fa\"\n",
    "INDEXED_BAMPATHS=\"/hackathon/Hackathon_Project_4/ENCODE_DATA_GM12878/COMPLETED/rg_bams/*bam\"\n",
    "CHROMOSOME=\"chr22\" # change this to any chromosome you like\n",
    "OUTPUT_DIRECTORY=\"/hackathon/Hackathon_Project_4/MPILEUP/rg_bams\"\n",
    "MPILEUP_COMMANDS_FILE=${OUTPUT_DIRECTORY}/\"mpileup_commands\"\n",
    "\n",
    "\n",
    "for bam in $(ls $INDEXED_BAMPATHS);\n",
    "do\n",
    "    # change the regex extension replacement \n",
    "    MPILEUP_COMMAND=\"samtools mpileup -d 1000000 --ff 1796 -q20  -Q10 -f $REFERENCE $bam -r $CHROMOSOME > ${OUTPUT_DIRECTORY}/$(basename $bam|sed 's/.bam/.mpileup/g') 2> ${OUTPUT_DIRECTORY}/$(basename $bam|sed 's/.bam/.log/g')\"\n",
    "    echo $MPILEUP_COMMAND\n",
    "done > $MPILEUP_COMMANDS_FILE\n",
    "\n",
    "wc -l ${MPILEUP_COMMANDS_FILE}\n",
    "head -n 5 ${MPILEUP_COMMANDS_FILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a commands file we can use with ``parallel``. Because this is already generated, I'm commenting out the line that executes this. Uncomment to re-run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "nproc \n",
    "CORENUM=32 # change number of cores here\n",
    "OUTPUT_DIRECTORY=\"/hackathon/Hackathon_Project_4/MPILEUP/rg_bams\"\n",
    "MPILEUP_COMMANDS_FILE=${OUTPUT_DIRECTORY}/\"mpileup_commands\"\n",
    "echo ${MPILEUP_COMMANDS_FILE}\n",
    "time cat ${MPILEUP_COMMANDS_FILE} |parallel --gnu -j $CORENUM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mpileup files should be generated in the output directory It took about an hour to run almost 500 samples with 32 cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTPUT_DIRECTORY=\"/hackathon/Hackathon_Project_4/MPILEUP/rg_bams\"\n",
    "ls ${OUTPUT_DIRECTORY}/*mpileup|wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity Checks\n",
    "-------------\n",
    "Spot check for a few high confidence variants...\n",
    "The two bams for initial testing, ENCFF000ARG, ENCFF000ARI are not included in the 479 so I couldn't reproduce the following with RG bams..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTPUT_DIRECTORY=\"/hackathon/Hackathon_Project_4/MPILEUP\" #\n",
    "BENCHMARK_VCF=\"/hackathon/Hackathon_Project_4/BENCHMARK/Benchmark-Test-V1.vcf\"\n",
    "\n",
    "# using only column 2 because we predefined a specific chromosome; \n",
    "# might want to use CHROM and POS columns from the VCFs otherwise\n",
    "grep -f <(cut -f 2 ${BENCHMARK_VCF}) ${OUTPUT_DIRECTORY}/ENCFF000ARG.mpileup > ${OUTPUT_DIRECTORY}/ENCFF000ARG.benchmarked.mpileup\n",
    "grep -f <(cut -f 2 ${BENCHMARK_VCF}) ${OUTPUT_DIRECTORY}/ENCFF000ARI.mpileup > ${OUTPUT_DIRECTORY}/ENCFF000ARI.benchmarked.mpileup\n",
    "wait\n",
    "wc -l ${OUTPUT_DIRECTORY}/ENCFF000ARG.benchmarked.mpileup\n",
    "wc -l ${OUTPUT_DIRECTORY}/ENCFF000ARI.benchmarked.mpileup\n",
    "wc -l ${BENCHMARK_VCF}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTPUT_DIRECTORY=\"/hackathon/Hackathon_Project_4/MPILEUP\"\n",
    "head ${OUTPUT_DIRECTORY}/ENCFF000ARG.benchmarked.mpileup\n",
    "echo\n",
    "head ${OUTPUT_DIRECTORY}/ENCFF000ARI.benchmarked.mpileup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAMTOOLS MPILEUP WITH A SINGLE MERGED BAM\n",
    "=========================================\n",
    "\n",
    "This merged bam ``/hackathon/Hackathon_Project_4/VariantCall_HAPLOTYPE/merged_rg.chr22.bam`` is constructed with ENCFF000ARG and ENCFF000ARI. The read groups are tagged so that we know the source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "REFERENCE=\"/hackathon/Hackathon_Project_4/REFERENCE_GENOME/hg19.fa\"\n",
    "bam=\"/hackathon/Hackathon_Project_4/VariantCall_HAPLOTYPE/merged_rg.chr22.bam\" # TODO: change this to bam when everything is indexed\n",
    "CHROMOSOME=\"chr22\" # change this to any chromosome you like\n",
    "OUTPUT_DIRECTORY=\"/hackathon/Hackathon_Project_4/MPILEUP/merged\"\n",
    "\n",
    "samtools mpileup -f $REFERENCE $(echo $bam|sed 's/.bai//g') -r $CHROMOSOME > ${OUTPUT_DIRECTORY}/$(basename $bam|sed 's/.bam/.mpileup/g')\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we pull out the subset of positions from the benchmark data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTPUT_DIRECTORY=\"/hackathon/Hackathon_Project_4/MPILEUP/merged\"\n",
    "BENCHMARK_VCF=\"/hackathon/Hackathon_Project_4/BENCHMARK/Benchmark-Test-V1.vcf\"\n",
    "\n",
    "grep -f <(cut -f 2 ${BENCHMARK_VCF}) ${OUTPUT_DIRECTORY}/merged_rg.chr22.mpileup > ${OUTPUT_DIRECTORY}/merged_rg.chr22.benchmarked.mpileup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMPARING COVERAGE BETWEEN THE TWO APPROACHES\n",
    "============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas\n",
    "\n",
    "out_dir = \"/hackathon/Hackathon_Project_4/MPILEUP\"\n",
    "ENCFF000ARG = os.path.join(out_dir, \"ENCFF000ARG.benchmarked.mpileup\")\n",
    "ENCFF000ARI = os.path.join(out_dir, \"ENCFF000ARI.benchmarked.mpileup\")\n",
    "merged = os.path.join(out_dir, \"merged\", \"merged_rg.chr22.benchmarked.mpileup\")\n",
    "\n",
    "ENCFF000ARG_data = pandas.read_table(ENCFF000ARG, header=None)\n",
    "ENCFF000ARI_data = pandas.read_table(ENCFF000ARI, header=None)\n",
    "merged_data = pandas.read_table(merged, header=None)\n",
    "assert all(ENCFF000ARG_data[1]==ENCFF000ARI_data[1])\n",
    "assert all(ENCFF000ARI_data[1]==merged_data[1])\n",
    "pandas.DataFrame({\"sum_of_cov\": ENCFF000ARG_data[3]+ENCFF000ARI_data[3], \"merged\": merged_data[3]})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUM OF COVERAGE\n",
    "==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "identifiers=\"/hackathon/Hackathon_Project_4/ENCODE_DATA_GM12878/COMPLETED/filesToUse.txt\"\n",
    "output_file=\"/hackathon/Hackathon_Project_4/MPILEUP/samples.txt\"\n",
    "with open(identifiers, 'r') as ifile, open(output_file, 'w') as ofile:\n",
    "    for row in ifile:\n",
    "        orow = os.path.join(\"/hackathon/Hackathon_Project_4/MPILEUP\", row.strip() + \".mpileup\" ) + \"\\n\"\n",
    "        ofile.write(orow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate a file with CHROM, POS-1, POS, sum of coverage\n",
    "--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 100000 lines...\n",
      "Parsed 200000 lines...\n",
      "Parsed 300000 lines...\n",
      "Parsed 400000 lines...\n",
      "Parsed 500000 lines...\n",
      "Parsed 600000 lines...\n",
      "Parsed 700000 lines...\n",
      "Parsed 800000 lines...\n",
      "Parsed 900000 lines...\n",
      "Parsed 1000000 lines...\n",
      "Parsed 1100000 lines...\n",
      "Parsed 1200000 lines...\n",
      "Parsed 1300000 lines...\n",
      "Parsed 1400000 lines...\n",
      "Parsed 1500000 lines...\n",
      "Parsed 1600000 lines...\n",
      "Parsed 1700000 lines...\n",
      "Parsed 1800000 lines...\n",
      "Parsed 1900000 lines...\n"
     ]
    }
   ],
   "source": [
    "# script to generate a file with CHROM, POS-1, POS, sum of coverage\n",
    "input_file=\"/hackathon/Hackathon_Project_4/DEPTH/HUGE.txt\"\n",
    "output_file=\"/hackathon/Hackathon_Project_4/DEPTH/HUGE_cov.txt\"\n",
    "\n",
    "import csv\n",
    "\n",
    "with open(input_file, 'r') as ifile, open(output_file, 'w') as ofile:\n",
    "    csvreader = csv.reader(ifile, delimiter=\"\\t\")\n",
    "    csvwriter = csv.writer(ofile, delimiter=\"\\t\")\n",
    "    counter = 0\n",
    "    for line in csvreader:\n",
    "        cov_sum = sum(map(lambda x: int(x) if x else 0,line[2:]))\n",
    "        csvwriter.writerow([ str(x) for x in [line[0], int(line[1])-1, line[1], cov_sum]])\n",
    "        counter += 1\n",
    "        if counter%100000==0:\n",
    "            print(\"Parsed {} lines...\".format(counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
