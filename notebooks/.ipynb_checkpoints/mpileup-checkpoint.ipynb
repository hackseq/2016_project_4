{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAMTOOLS MPILEUP ON MULTIPLE BAMS\n",
    "================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! samtools --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See ``mpileup`` [manual](http://samtools.sourceforge.net/mpileup.shtml).\n",
    "``samtools mpileup`` requires:\n",
    "- an index reference fasta\n",
    "- indexed bam files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input bam files used in this example are assembled on hg19. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create a commands file for parallelization. We are using chromosome 22 as an example here. \n",
    "Parameters similar to GATK's default read filters were applied:\n",
    "- depth 1000000 \n",
    "- mapping quality 20 ([GATK haplotype caller](https://software.broadinstitute.org/gatk/gatkdocs/org_broadinstitute_gatk_tools_walkers_haplotypecaller_HaplotypeCaller.php) default)\n",
    "- base quality 10 ([GATK haplotype caller](https://software.broadinstitute.org/gatk/gatkdocs/org_broadinstitute_gatk_tools_walkers_haplotypecaller_HaplotypeCaller.php) default)\n",
    "- exclude read unmapped, not primary alignment, read fails platform/vendor quality checks, read is PCR or optical duplicate ([GATK haplotype caller](https://software.broadinstitute.org/gatk/gatkdocs/org_broadinstitute_gatk_tools_walkers_haplotypecaller_HaplotypeCaller.php) default)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "REFERENCE=\"/hackathon/Hackathon_Project_4/REFERENCE_GENOME/hg19.fa\"\n",
    "INDEXED_BAMPATHS=\"/hackathon/Hackathon_Project_4/ENCODE_DATA_GM12878/COMPLETED/rg_bams/*bam\"\n",
    "CHROMOSOME=\"chr22\" # change this to any chromosome you like\n",
    "OUTPUT_DIRECTORY=\"/hackathon/Hackathon_Project_4/MPILEUP/rg_bams\"\n",
    "MPILEUP_COMMANDS_FILE=${OUTPUT_DIRECTORY}/\"mpileup_commands\"\n",
    "\n",
    "\n",
    "for bam in $(ls $INDEXED_BAMPATHS);\n",
    "do\n",
    "    # change the regex extension replacement \n",
    "    MPILEUP_COMMAND=\"samtools mpileup -d 1000000 --ff 1796 -q20  -Q10 -f $REFERENCE $bam -r $CHROMOSOME > ${OUTPUT_DIRECTORY}/$(basename $bam|sed 's/.bam/.mpileup/g') 2> ${OUTPUT_DIRECTORY}/$(basename $bam|sed 's/.bam/.log/g')\"\n",
    "    echo $MPILEUP_COMMAND\n",
    "done > $MPILEUP_COMMANDS_FILE\n",
    "\n",
    "wc -l ${MPILEUP_COMMANDS_FILE}\n",
    "head -n 5 ${MPILEUP_COMMANDS_FILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a commands file we can use with ``parallel``. Because this is already generated, I'm commenting out the line that executes this. Uncomment to re-run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "nproc \n",
    "CORENUM=32 # change number of cores here\n",
    "OUTPUT_DIRECTORY=\"/hackathon/Hackathon_Project_4/MPILEUP/rg_bams\"\n",
    "MPILEUP_COMMANDS_FILE=${OUTPUT_DIRECTORY}/\"mpileup_commands\"\n",
    "echo ${MPILEUP_COMMANDS_FILE}\n",
    "time cat ${MPILEUP_COMMANDS_FILE} |parallel --gnu -j $CORENUM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mpileup files should be generated in the output directory It took about an hour to run almost 500 samples with 32 cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTPUT_DIRECTORY=\"/hackathon/Hackathon_Project_4/MPILEUP/rg_bams\"\n",
    "ls ${OUTPUT_DIRECTORY}/*mpileup|wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity Checks\n",
    "-------------\n",
    "Spot check for a few high confidence variants...\n",
    "The two bams for initial testing, ENCFF000ARG, ENCFF000ARI are not included in the 479 so I couldn't reproduce the following with RG bams..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTPUT_DIRECTORY=\"/hackathon/Hackathon_Project_4/MPILEUP\" #\n",
    "BENCHMARK_VCF=\"/hackathon/Hackathon_Project_4/BENCHMARK/Benchmark-Test-V1.vcf\"\n",
    "\n",
    "# using only column 2 because we predefined a specific chromosome; \n",
    "# might want to use CHROM and POS columns from the VCFs otherwise\n",
    "grep -f <(cut -f 2 ${BENCHMARK_VCF}) ${OUTPUT_DIRECTORY}/ENCFF000ARG.mpileup > ${OUTPUT_DIRECTORY}/ENCFF000ARG.benchmarked.mpileup\n",
    "grep -f <(cut -f 2 ${BENCHMARK_VCF}) ${OUTPUT_DIRECTORY}/ENCFF000ARI.mpileup > ${OUTPUT_DIRECTORY}/ENCFF000ARI.benchmarked.mpileup\n",
    "wait\n",
    "wc -l ${OUTPUT_DIRECTORY}/ENCFF000ARG.benchmarked.mpileup\n",
    "wc -l ${OUTPUT_DIRECTORY}/ENCFF000ARI.benchmarked.mpileup\n",
    "wc -l ${BENCHMARK_VCF}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTPUT_DIRECTORY=\"/hackathon/Hackathon_Project_4/MPILEUP\"\n",
    "head ${OUTPUT_DIRECTORY}/ENCFF000ARG.benchmarked.mpileup\n",
    "echo\n",
    "head ${OUTPUT_DIRECTORY}/ENCFF000ARI.benchmarked.mpileup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAMTOOLS MPILEUP WITH A SINGLE MERGED BAM\n",
    "=========================================\n",
    "\n",
    "This merged bam ``/hackathon/Hackathon_Project_4/VariantCall_HAPLOTYPE/merged_rg.chr22.bam`` is constructed with ENCFF000ARG and ENCFF000ARI. The read groups are tagged so that we know the source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "REFERENCE=\"/hackathon/Hackathon_Project_4/REFERENCE_GENOME/hg19.fa\"\n",
    "bam=\"/hackathon/Hackathon_Project_4/VariantCall_HAPLOTYPE/merged_rg.chr22.bam\" # TODO: change this to bam when everything is indexed\n",
    "CHROMOSOME=\"chr22\" # change this to any chromosome you like\n",
    "OUTPUT_DIRECTORY=\"/hackathon/Hackathon_Project_4/MPILEUP/merged\"\n",
    "\n",
    "samtools mpileup -f $REFERENCE $(echo $bam|sed 's/.bai//g') -r $CHROMOSOME > ${OUTPUT_DIRECTORY}/$(basename $bam|sed 's/.bam/.mpileup/g')\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we pull out the subset of positions from the benchmark data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTPUT_DIRECTORY=\"/hackathon/Hackathon_Project_4/MPILEUP/merged\"\n",
    "BENCHMARK_VCF=\"/hackathon/Hackathon_Project_4/BENCHMARK/Benchmark-Test-V1.vcf\"\n",
    "\n",
    "grep -f <(cut -f 2 ${BENCHMARK_VCF}) ${OUTPUT_DIRECTORY}/merged_rg.chr22.mpileup > ${OUTPUT_DIRECTORY}/merged_rg.chr22.benchmarked.mpileup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMPARING COVERAGE BETWEEN THE TWO APPROACHES\n",
    "============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas\n",
    "\n",
    "out_dir = \"/hackathon/Hackathon_Project_4/MPILEUP\"\n",
    "ENCFF000ARG = os.path.join(out_dir, \"ENCFF000ARG.benchmarked.mpileup\")\n",
    "ENCFF000ARI = os.path.join(out_dir, \"ENCFF000ARI.benchmarked.mpileup\")\n",
    "merged = os.path.join(out_dir, \"merged\", \"merged_rg.chr22.benchmarked.mpileup\")\n",
    "\n",
    "ENCFF000ARG_data = pandas.read_table(ENCFF000ARG, header=None)\n",
    "ENCFF000ARI_data = pandas.read_table(ENCFF000ARI, header=None)\n",
    "merged_data = pandas.read_table(merged, header=None)\n",
    "assert all(ENCFF000ARG_data[1]==ENCFF000ARI_data[1])\n",
    "assert all(ENCFF000ARI_data[1]==merged_data[1])\n",
    "pandas.DataFrame({\"sum_of_cov\": ENCFF000ARG_data[3]+ENCFF000ARI_data[3], \"merged\": merged_data[3]})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUM OF COVERAGE\n",
    "==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "identifiers=\"/hackathon/Hackathon_Project_4/ENCODE_DATA_GM12878/COMPLETED/filesToUse.txt\"\n",
    "output_file=\"/hackathon/Hackathon_Project_4/MPILEUP/samples.txt\"\n",
    "with open(identifiers, 'r') as ifile, open(output_file, 'w') as ofile:\n",
    "    for row in ifile:\n",
    "        orow = os.path.join(\"/hackathon/Hackathon_Project_4/MPILEUP\", row.strip() + \".mpileup\" ) + \"\\n\"\n",
    "        ofile.write(orow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate a file with CHROM, POS, POS+1, sum of coverage\n",
    "--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 100000 lines...\n",
      "Parsed 200000 lines...\n",
      "Parsed 300000 lines...\n",
      "Parsed 400000 lines...\n",
      "Parsed 500000 lines...\n",
      "Parsed 600000 lines...\n",
      "Parsed 700000 lines...\n",
      "Parsed 800000 lines...\n",
      "Parsed 900000 lines...\n",
      "Parsed 1000000 lines...\n",
      "Parsed 1100000 lines...\n",
      "Parsed 1200000 lines...\n",
      "Parsed 1300000 lines...\n",
      "Parsed 1400000 lines...\n",
      "Parsed 1500000 lines...\n",
      "Parsed 1600000 lines...\n",
      "Parsed 1700000 lines...\n",
      "Parsed 1800000 lines...\n",
      "Parsed 1900000 lines...\n",
      "Parsed 2000000 lines...\n",
      "Parsed 2100000 lines...\n",
      "Parsed 2200000 lines...\n",
      "Parsed 2300000 lines...\n",
      "Parsed 2400000 lines...\n",
      "Parsed 2500000 lines...\n",
      "Parsed 2600000 lines...\n",
      "Parsed 2700000 lines...\n",
      "Parsed 2800000 lines...\n",
      "Parsed 2900000 lines...\n",
      "Parsed 3000000 lines...\n",
      "Parsed 3100000 lines...\n",
      "Parsed 3200000 lines...\n",
      "Parsed 3300000 lines...\n",
      "Parsed 3400000 lines...\n",
      "Parsed 3500000 lines...\n",
      "Parsed 3600000 lines...\n",
      "Parsed 3700000 lines...\n",
      "Parsed 3800000 lines...\n",
      "Parsed 3900000 lines...\n",
      "Parsed 4000000 lines...\n",
      "Parsed 4100000 lines...\n",
      "Parsed 4200000 lines...\n",
      "Parsed 4300000 lines...\n",
      "Parsed 4400000 lines...\n",
      "Parsed 4500000 lines...\n",
      "Parsed 4600000 lines...\n",
      "Parsed 4700000 lines...\n",
      "Parsed 4800000 lines...\n",
      "Parsed 4900000 lines...\n",
      "Parsed 5000000 lines...\n",
      "Parsed 5100000 lines...\n",
      "Parsed 5200000 lines...\n",
      "Parsed 5300000 lines...\n",
      "Parsed 5400000 lines...\n",
      "Parsed 5500000 lines...\n",
      "Parsed 5600000 lines...\n",
      "Parsed 5700000 lines...\n",
      "Parsed 5800000 lines...\n",
      "Parsed 5900000 lines...\n",
      "Parsed 6000000 lines...\n",
      "Parsed 6100000 lines...\n",
      "Parsed 6200000 lines...\n",
      "Parsed 6300000 lines...\n",
      "Parsed 6400000 lines...\n",
      "Parsed 6500000 lines...\n",
      "Parsed 6600000 lines...\n",
      "Parsed 6700000 lines...\n",
      "Parsed 6800000 lines...\n",
      "Parsed 6900000 lines...\n",
      "Parsed 7000000 lines...\n",
      "Parsed 7100000 lines...\n",
      "Parsed 7200000 lines...\n",
      "Parsed 7300000 lines...\n",
      "Parsed 7400000 lines...\n",
      "Parsed 7500000 lines...\n",
      "Parsed 7600000 lines...\n",
      "Parsed 7700000 lines...\n",
      "Parsed 7800000 lines...\n",
      "Parsed 7900000 lines...\n",
      "Parsed 8000000 lines...\n",
      "Parsed 8100000 lines...\n",
      "Parsed 8200000 lines...\n",
      "Parsed 8300000 lines...\n",
      "Parsed 8400000 lines...\n",
      "Parsed 8500000 lines...\n",
      "Parsed 8600000 lines...\n",
      "Parsed 8700000 lines...\n",
      "Parsed 8800000 lines...\n",
      "Parsed 8900000 lines...\n",
      "Parsed 9000000 lines...\n",
      "Parsed 9100000 lines...\n",
      "Parsed 9200000 lines...\n",
      "Parsed 9300000 lines...\n",
      "Parsed 9400000 lines...\n",
      "Parsed 9500000 lines...\n"
     ]
    }
   ],
   "source": [
    "# script to generate a file with CHROM, POS, POS+1, sum of coverage\n",
    "input_file=\"/hackathon/Hackathon_Project_4/DEPTH/HUGE.txt\"\n",
    "output_file=\"/hackathon/Hackathon_Project_4/DEPTH/HUGE_cov.txt\"\n",
    "\n",
    "import csv\n",
    "\n",
    "with open(input_file, 'r') as ifile, open(output_file, 'w') as ofile:\n",
    "    csvreader = csv.reader(ifile, delimiter=\"\\t\")\n",
    "    csvwriter = csv.writer(ofile, delimiter=\"\\t\")\n",
    "    counter = 0\n",
    "    for line in csvreader:\n",
    "        cov_sum = sum(map(lambda x: int(x) if x else 0,line[3:]))\n",
    "        csvwriter.writerow([ str(x) for x in [line[0], int(line[1]), int(line[2]), cov_sum]])\n",
    "        counter += 1\n",
    "        if counter%100000==0:\n",
    "            print(\"Parsed {} lines...\".format(counter))\n",
    "            \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
